# LLM í˜¸ì¶œ ì•„í‚¤í…ì²˜ ìƒì„¸ ì„¤ê³„

> Phase 10-2ì˜ í•µì‹¬ â€” ì „ì²´ í”„ë¡œì íŠ¸ ê³µìš© LLM ì—°ë™ ê¸°ë°˜
> ì‘ì„±: 2026-02-15

---

## 1. í˜¸ì¶œ ìš°ì„ ìˆœìœ„ (4ë‹¨ í´ë°±)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  classical-text-platform (Python/FastAPI)                    â”‚
â”‚                                                             â”‚
â”‚  src/llm/router.py  â† LLM í˜¸ì¶œì˜ ë‹¨ì¼ ì§„ì…ì                â”‚
â”‚      â”‚                                                      â”‚
â”‚      â”œâ”€ 1ìˆœìœ„: Base44 InvokeLLM (agent-chat HTTP)           â”‚
â”‚      â”‚   localhost:8787/api/chat                            â”‚
â”‚      â”‚   ì¡°ê±´: agent-chat ì„œë²„ê°€ ì‹¤í–‰ ì¤‘                     â”‚
â”‚      â”‚   ì¥ì : ë¬´ë£Œ, ì´ë¯¸ì§€ ë¶„ì„ ê°€ëŠ¥, MCP ë„êµ¬ ì—°ë™        â”‚
â”‚      â”‚                                                      â”‚
â”‚      â”œâ”€ 2ìˆœìœ„: Base44 InvokeLLM (Node.js bridge)            â”‚
â”‚      â”‚   subprocess: node src/llm/bridge/invoke.js          â”‚
â”‚      â”‚   ì¡°ê±´: Node.js + backend-44 ì„¤ì¹˜ë¨                  â”‚
â”‚      â”‚   ì¥ì : ì„œë²„ ì—†ì´ SDK ì§ì ‘ ì‚¬ìš©, 1íšŒì„± í˜¸ì¶œ          â”‚
â”‚      â”‚                                                      â”‚
â”‚      â”œâ”€ 3ìˆœìœ„: Ollama (ë¡œì»¬ ì„œë²„)                            â”‚
â”‚      â”‚   localhost:11434/api/generate                       â”‚
â”‚      â”‚   ëª¨ë¸: qwen3-vl:235b-cloud, kimi-k2.5:cloud,       â”‚
â”‚      â”‚         minimax-m2.5:cloud, glm-5:cloud,             â”‚
â”‚      â”‚         gemini-3-flash-preview:cloud                 â”‚
â”‚      â”‚   ì¡°ê±´: Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘                         â”‚
â”‚      â”‚   ì¥ì : í´ë¼ìš°ë“œ ëª¨ë¸ì„ ë¡œì»¬ í”„ë¡ì‹œë¡œ, ë¹„ì „ ëª¨ë¸ ì§€ì› â”‚
â”‚      â”‚                                                      â”‚
â”‚      â””â”€ 4ìˆœìœ„: ì§ì ‘ API í˜¸ì¶œ                                 â”‚
â”‚          Anthropic / OpenAI / Google Gemini                  â”‚
â”‚          ì¡°ê±´: API í‚¤ê°€ ì„¤ì •ë¨                               â”‚
â”‚          ì¥ì : ê°€ì¥ ì•ˆì •ì , ìµœì‹  ëª¨ë¸                        â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ì™œ ì´ ìˆœì„œì¸ê°€

| ìˆœìœ„ | ë°©ì‹ | ë¹„ìš© | ì´ë¯¸ì§€ ë¶„ì„ | ì˜ì¡´ì„± | ì˜¤í”„ë¼ì¸ |
|------|------|------|-------------|--------|----------|
| 1 | Base44 agent-chat | ë¬´ë£Œ | âœ… (UploadFile) | agent-chat ì‹¤í–‰ | âœ— |
| 2 | Base44 bridge | ë¬´ë£Œ | âœ… (UploadFile) | Node.js ì„¤ì¹˜ | âœ— |
| 3 | Ollama í´ë¼ìš°ë“œ ëª¨ë¸ | ë¬´ë£Œ~ì €ê°€ | âœ… (qwen3-vl ë“±) | Ollama ì‹¤í–‰ | â–³ (ë¡œì»¬ ëª¨ë¸ì€ ê°€ëŠ¥) |
| 4 | ì§ì ‘ API | ìœ ë£Œ | âœ… (Claude/Gemini) | API í‚¤ | âœ— |

- 1Â·2ìˆœìœ„ëŠ” Base44ì˜ ë¬´ë£Œ LLMì„ ìµœëŒ€í•œ í™œìš©
- 3ìˆœìœ„ëŠ” Ollamaë¥¼ í†µí•œ í´ë¼ìš°ë“œ ëª¨ë¸ í”„ë¡ì‹œ (ë¹„ìš© íš¨ìœ¨)
- 4ìˆœìœ„ëŠ” ìµœí›„ ìˆ˜ë‹¨ (ìœ ë£Œì§€ë§Œ ê°€ì¥ ì•ˆì •ì )

---

## 2. ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
src/
  llm/
    __init__.py
    router.py           â† ë‹¨ì¼ ì§„ì…ì : call_llm(), call_llm_with_image()
    providers/
      __init__.py
      base.py           â† BaseLlmProvider ì¶”ìƒ í´ë˜ìŠ¤
      base44_http.py    â† 1ìˆœìœ„: agent-chat HTTP
      base44_bridge.py  â† 2ìˆœìœ„: Node.js bridge subprocess
      ollama.py         â† 3ìˆœìœ„: Ollama REST API
      anthropic.py      â† 4ìˆœìœ„: Claude API
      openai.py         â† 4ìˆœìœ„: OpenAI API
      gemini.py         â† 4ìˆœìœ„: Gemini API
    bridge/
      invoke.js         â† Node.js ë¸Œë¦¿ì§€ ìŠ¤í¬ë¦½íŠ¸ (backend-44 SDK ì‚¬ìš©)
      invoke_vision.js  â† ì´ë¯¸ì§€ ë¶„ì„ìš© ë¸Œë¦¿ì§€
      package.json      â† ìµœì†Œ ì˜ì¡´ì„± (backend-44/src/client.js ì°¸ì¡°)
    prompts/
      layout_analysis.yaml
      translation.yaml
      annotation.yaml
    config.py           â† ì„¤ì •: API í‚¤, ëª¨ë¸ ì„ íƒ, ìš°ì„ ìˆœìœ„
    usage_tracker.py    â† ë¹„ìš© ì¶”ì 
    draft.py            â† LlmDraft ëª¨ë¸ (Draft â†’ Review â†’ Commit)
```

---

## 3. í•µì‹¬ ì¸í„°í˜ì´ìŠ¤

### 3.1 BaseLlmProvider

```python
# src/llm/providers/base.py

from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Optional


@dataclass
class LlmResponse:
    """LLM ì‘ë‹µ í†µí•© ëª¨ë¸.
    
    ëª¨ë“  providerê°€ ì´ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•œë‹¤.
    ì–´ë–¤ providerë¥¼ ì¼ë“  í˜¸ì¶œìëŠ” ë™ì¼í•œ í˜•ì‹ì„ ë°›ëŠ”ë‹¤.
    """
    text: str                    # ì‘ë‹µ í…ìŠ¤íŠ¸
    provider: str                # "base44_http", "ollama", "anthropic" ë“±
    model: str                   # ì‹¤ì œ ì‚¬ìš©ëœ ëª¨ë¸ëª…
    tokens_in: int | None        # ì…ë ¥ í† í° (ì¶”ì • ê°€ëŠ¥í•  ë•Œ)
    tokens_out: int | None       # ì¶œë ¥ í† í°
    cost_usd: float | None       # ì¶”ì • ë¹„ìš© (ë¬´ë£Œë©´ 0.0)
    raw: dict | None             # providerë³„ ì›ë³¸ ì‘ë‹µ (ë””ë²„ê¹…ìš©)


class BaseLlmProvider(ABC):
    """LLM provider ì¶”ìƒ í´ë˜ìŠ¤.
    
    ê° providerëŠ” ì´ê²ƒì„ êµ¬í˜„í•œë‹¤.
    router.pyê°€ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ìˆœì„œëŒ€ë¡œ ì‹œë„.
    """
    
    provider_id: str        # "base44_http", "base44_bridge", "ollama", ...
    display_name: str       # "Base44 (agent-chat)", ...
    supports_image: bool    # ì´ë¯¸ì§€ ì…ë ¥ ê°€ëŠ¥ ì—¬ë¶€
    
    @abstractmethod
    async def is_available(self) -> bool:
        """ì´ providerê°€ í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸.
        
        - base44_http: localhost:8787 í—¬ìŠ¤ì²´í¬
        - base44_bridge: Node.js + backend-44 ê²½ë¡œ ì¡´ì¬ í™•ì¸
        - ollama: localhost:11434 í—¬ìŠ¤ì²´í¬
        - anthropic: API í‚¤ ì¡´ì¬ í™•ì¸
        """
        ...
    
    @abstractmethod
    async def call(
        self,
        prompt: str,
        *,
        system: str | None = None,
        response_format: str = "text",  # "text" | "json"
        model: str | None = None,       # ëª¨ë¸ ì˜¤ë²„ë¼ì´ë“œ
        max_tokens: int = 4096,
    ) -> LlmResponse:
        """í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ LLM í˜¸ì¶œ."""
        ...
    
    @abstractmethod
    async def call_with_image(
        self,
        prompt: str,
        image: bytes,
        *,
        image_mime: str = "image/png",
        system: str | None = None,
        response_format: str = "text",
        model: str | None = None,
        max_tokens: int = 4096,
    ) -> LlmResponse:
        """ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ LLM í˜¸ì¶œ.
        
        ë ˆì´ì•„ì›ƒ ë¶„ì„(10-2), OCR ë³´ì¡° ë“±ì—ì„œ ì‚¬ìš©.
        supports_image=Falseì¸ providerì—ì„œ í˜¸ì¶œí•˜ë©´ NotImplementedError.
        """
        ...
```

### 3.2 Router (ë‹¨ì¼ ì§„ì…ì )

```python
# src/llm/router.py

class LlmRouter:
    """LLM í˜¸ì¶œì˜ ë‹¨ì¼ ì§„ì…ì .
    
    ìš°ì„ ìˆœìœ„ì— ë”°ë¼ providerë¥¼ ìˆœì„œëŒ€ë¡œ ì‹œë„í•œë‹¤.
    ëª¨ë“  ì½”ë“œì—ì„œ LLMì´ í•„ìš”í•˜ë©´ ì´ê²ƒë§Œ í˜¸ì¶œí•˜ë©´ ëœë‹¤.
    
    ì‚¬ìš© ì˜ˆì‹œ:
        router = LlmRouter(config)
        
        # í…ìŠ¤íŠ¸ í˜¸ì¶œ
        response = await router.call("ì´ ë¬¸ì¥ì„ ë²ˆì—­í•´ì¤˜")
        
        # ì´ë¯¸ì§€ í˜¸ì¶œ (ë ˆì´ì•„ì›ƒ ë¶„ì„)
        response = await router.call_with_image(
            "ì´ í˜ì´ì§€ì˜ ì˜ì—­ì„ ë¶„ì„í•´ì¤˜",
            image_bytes
        )
    """
    
    def __init__(self, config: LlmConfig):
        # ìš°ì„ ìˆœìœ„ ìˆœì„œëŒ€ë¡œ provider ëª©ë¡ ìƒì„±
        self.providers = [
            Base44HttpProvider(config),     # 1ìˆœìœ„
            Base44BridgeProvider(config),   # 2ìˆœìœ„
            OllamaProvider(config),         # 3ìˆœìœ„
            AnthropicProvider(config),      # 4ìˆœìœ„
            OpenAIProvider(config),         # 4ìˆœìœ„
            GeminiProvider(config),         # 4ìˆœìœ„
        ]
        self.usage_tracker = UsageTracker(config)
    
    async def call(self, prompt, *, system=None, response_format="text",
                   require_image=False,
                   force_provider=None, force_model=None,
                   purpose="text", **kwargs) -> LlmResponse:
        """LLM í˜¸ì¶œ â€” ë‹¨ì¼ ì§„ì…ì .
        
        ê¸°ë³¸ ë™ì‘: ìš°ì„ ìˆœìœ„ì— ë”°ë¼ providerë¥¼ ìˆœì„œëŒ€ë¡œ ì‹œë„.
        
        ëª¨ë¸ ì„ íƒ ì˜µì…˜ (í’ˆì§ˆ í…ŒìŠ¤íŠ¸Â·ë¹„êµìš©):
          force_provider: íŠ¹ì • providerë§Œ ì‚¬ìš©
            ì˜ˆ: "ollama", "anthropic", "base44_http"
          force_model: íŠ¹ì • ëª¨ë¸ ì§€ì • (force_providerì™€ í•¨ê»˜ ì‚¬ìš©)
            ì˜ˆ: "qwen3-vl:235b-cloud", "claude-sonnet-4-20250514"
        
        ì‚¬ìš© ì˜ˆì‹œ:
          # ê¸°ë³¸: í´ë°± ìˆœì„œëŒ€ë¡œ
          await router.call("ë²ˆì—­í•´ì¤˜")
          
          # Ollamaì˜ íŠ¹ì • ëª¨ë¸ë¡œ ê°•ì œ ì§€ì •
          await router.call("ë²ˆì—­í•´ì¤˜",
              force_provider="ollama",
              force_model="glm-5:cloud")
          
          # Claudeë¡œ ê°•ì œ ì§€ì • (í’ˆì§ˆ ë¹„êµìš©)
          await router.call("ë²ˆì—­í•´ì¤˜",
              force_provider="anthropic")
        """
        # â”€â”€ ëª…ì‹œì  ëª¨ë¸ ì„ íƒ ëª¨ë“œ â”€â”€
        if force_provider:
            provider = self._get_provider(force_provider)
            if not provider:
                raise LlmProviderError(
                    f"provider '{force_provider}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                    f"ì‚¬ìš© ê°€ëŠ¥: {[p.provider_id for p in self.providers]}"
                )
            if not await provider.is_available():
                raise LlmProviderError(
                    f"provider '{force_provider}'ê°€ í˜„ì¬ ì‚¬ìš© ë¶ˆê°€í•©ë‹ˆë‹¤."
                )
            if require_image and not provider.supports_image:
                raise LlmProviderError(
                    f"provider '{force_provider}'ëŠ” ì´ë¯¸ì§€ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                )
            
            response = await provider.call(
                prompt, system=system,
                response_format=response_format,
                model=force_model, **kwargs
            )
            self.usage_tracker.log(response, purpose=purpose)
            return response
        
        # â”€â”€ ìë™ í´ë°± ëª¨ë“œ â”€â”€
        errors = []
        
        for provider in self.providers:
            if require_image and not provider.supports_image:
                continue
            
            try:
                if not await provider.is_available():
                    continue
                
                response = await provider.call(
                    prompt, system=system,
                    response_format=response_format,
                    purpose=purpose, **kwargs
                )
                
                self.usage_tracker.log(response, purpose=purpose)
                return response
                
            except Exception as e:
                errors.append(f"{provider.provider_id}: {e}")
                continue
        
        raise LlmUnavailableError(
            "ì‚¬ìš© ê°€ëŠ¥í•œ LLM providerê°€ ì—†ìŠµë‹ˆë‹¤.\n"
            "ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ í™•ì¸í•˜ì„¸ìš”:\n"
            "1. agent-chat ì„œë²„ ì‹¤í–‰ (npm run agent:chat)\n"
            "2. Ollama ì‹¤í–‰ (ollama serve)\n"
            "3. API í‚¤ ì„¤ì • (.envì— ANTHROPIC_API_KEY ë“±)\n\n"
            f"ì‹œë„í•œ providerë³„ ì˜¤ë¥˜:\n" + "\n".join(errors)
        )
    
    async def call_with_image(self, prompt, image, **kwargs) -> LlmResponse:
        """ì´ë¯¸ì§€ ë¶„ì„ í˜¸ì¶œ. supports_imageì¸ providerë§Œ ì‹œë„.
        
        force_provider, force_modelë„ ì§€ì› â€” kwargsë¡œ ì „ë‹¬.
        """
        return await self.call(
            prompt, require_image=True, _image=image, **kwargs
        )
    
    async def compare(
        self,
        prompt: str,
        *,
        targets: list[str | tuple[str, str]] | None = None,
        image: bytes | None = None,
        system: str | None = None,
        purpose: str = "comparison",
        **kwargs,
    ) -> list[LlmResponse | Exception]:
        """ì—¬ëŸ¬ ëª¨ë¸ì— ê°™ì€ ì…ë ¥ì„ ë³´ë‚´ì„œ ê²°ê³¼ë¥¼ ë¹„êµ.
        
        í’ˆì§ˆ í…ŒìŠ¤íŠ¸ìš©. ê²°ê³¼ë¥¼ ë‚˜ë€íˆ ë³´ì—¬ì¤˜ì„œ ì–´ë–¤ ëª¨ë¸ì´ ë‚˜ì€ì§€ íŒë‹¨.
        
        targets: ë¹„êµí•  provider(+ëª¨ë¸) ëª©ë¡
          - ë¬¸ìì—´: provider_id â†’ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
          - íŠœí”Œ: (provider_id, model) â†’ íŠ¹ì • ëª¨ë¸ ì§€ì •
          - None â†’ í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  provider
        
        ì‚¬ìš© ì˜ˆì‹œ:
          # ê°€ìš©í•œ ì „ì²´ ëª¨ë¸ ë¹„êµ
          results = await router.compare("ì´ í˜ì´ì§€ ë¶„ì„í•´ì¤˜", image=img)
          
          # íŠ¹ì • ëª¨ë¸ë§Œ ë¹„êµ
          results = await router.compare(
              "ì´ ë¬¸ì¥ ë²ˆì—­í•´ì¤˜",
              targets=[
                  "base44_http",
                  ("ollama", "glm-5:cloud"),
                  ("ollama", "kimi-k2.5:cloud"),
                  "anthropic",
              ]
          )
          
        ë°˜í™˜: LlmResponse ë¦¬ìŠ¤íŠ¸ (ì‹¤íŒ¨í•œ ê²ƒì€ Exception ê°ì²´)
        """
        import asyncio
        
        # ë¹„êµ ëŒ€ìƒ ê²°ì •
        if targets is None:
            # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  provider
            pairs = []
            for p in self.providers:
                if image and not p.supports_image:
                    continue
                if await p.is_available():
                    pairs.append((p.provider_id, None))
        else:
            pairs = []
            for t in targets:
                if isinstance(t, str):
                    pairs.append((t, None))
                else:
                    pairs.append((t[0], t[1]))
        
        # ë³‘ë ¬ í˜¸ì¶œ
        async def _call_one(provider_id, model):
            try:
                return await self.call(
                    prompt,
                    system=system,
                    force_provider=provider_id,
                    force_model=model,
                    purpose=purpose,
                    **kwargs,
                )
            except Exception as e:
                return e
        
        tasks = [_call_one(pid, model) for pid, model in pairs]
        results = await asyncio.gather(*tasks)
        
        # ë¹„êµ ê¸°ë¡ (usage_trackerì— comparison ë¡œê·¸)
        self.usage_tracker.log_comparison(
            purpose=purpose,
            targets=pairs,
            results=results,
        )
        
        return results
    
    async def get_available_models(self) -> list[dict]:
        """í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  providerì™€ ëª¨ë¸ ëª©ë¡.
        
        GUIì˜ ëª¨ë¸ ì„ íƒ ë“œë¡­ë‹¤ìš´ì—ì„œ ì‚¬ìš©.
        
        ë°˜í™˜ ì˜ˆì‹œ:
        [
            {"provider": "base44_http", "model": "(ìë™)", "available": True,
             "display": "Base44 InvokeLLM", "cost": "ë¬´ë£Œ"},
            {"provider": "ollama", "model": "qwen3-vl:235b-cloud", "available": True,
             "display": "Ollama â€” qwen3-vl (ë¹„ì „)", "cost": "ë¬´ë£Œ", "vision": True},
            {"provider": "ollama", "model": "kimi-k2.5:cloud", "available": True,
             "display": "Ollama â€” kimi-k2.5", "cost": "ë¬´ë£Œ"},
            {"provider": "anthropic", "model": "claude-sonnet-4-20250514", "available": False,
             "display": "Claude Sonnet 4", "cost": "ìœ ë£Œ", "reason": "API í‚¤ ë¯¸ì„¤ì •"},
        ]
        """
        models = []
        
        for provider in self.providers:
            available = await provider.is_available()
            
            if provider.provider_id == "ollama" and available:
                # Ollama: ì‹¤ì œ ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
                ollama_models = await provider.list_models()
                for m in ollama_models:
                    models.append({
                        "provider": "ollama",
                        "model": m["name"],
                        "available": True,
                        "display": f"Ollama â€” {m['name']}",
                        "cost": "ë¬´ë£Œ",
                        "vision": "vl" in m["name"].lower(),
                    })
            else:
                models.append({
                    "provider": provider.provider_id,
                    "model": getattr(provider, "DEFAULT_MODEL", "(ìë™)"),
                    "available": available,
                    "display": provider.display_name,
                    "cost": "ë¬´ë£Œ" if provider.provider_id.startswith("base44") or provider.provider_id == "ollama" else "ìœ ë£Œ",
                    "vision": provider.supports_image,
                })
        
        return models
    
    def _get_provider(self, provider_id: str) -> BaseLlmProvider | None:
        """provider_idë¡œ provider ê°ì²´ë¥¼ ì°¾ëŠ”ë‹¤."""
        for p in self.providers:
            if p.provider_id == provider_id:
                return p
        return None
```

---

## 4. Providerë³„ êµ¬í˜„ ìƒì„¸

### 4.1 Base44 HTTP (1ìˆœìœ„)

```python
# src/llm/providers/base44_http.py

class Base44HttpProvider(BaseLlmProvider):
    """agent-chat ì„œë²„(localhost:8787)ë¥¼ í†µí•œ Base44 InvokeLLM í˜¸ì¶œ.
    
    backend-44ì˜ agent-chatì´ ì‹¤í–‰ ì¤‘ì¼ ë•Œ ì‚¬ìš©.
    ì¥ì : ë¬´ë£Œ, MCP ë„êµ¬ ì—°ë™, ì„¸ì…˜ ê´€ë¦¬.
    
    í˜¸ì¶œ íë¦„:
      Python â†’ HTTP POST localhost:8787/api/chat
            â†’ agent-chat â†’ Base44 InvokeLLM
            â†’ ê²°ê³¼ JSON ë°˜í™˜
    """
    
    provider_id = "base44_http"
    display_name = "Base44 (agent-chat)"
    supports_image = True  # agent-chatì´ ì²¨ë¶€íŒŒì¼ì„ ì§€ì›
    
    AGENT_CHAT_URL = "http://127.0.0.1:8787"
    
    async def is_available(self) -> bool:
        """agent-chat ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸."""
        try:
            async with httpx.AsyncClient(timeout=2.0) as client:
                resp = await client.get(f"{self.AGENT_CHAT_URL}/api/meta")
                return resp.status_code == 200
        except (httpx.ConnectError, httpx.TimeoutException):
            return False
    
    async def call(self, prompt, *, system=None, response_format="text",
                   connector="sequential-thinking", **kwargs) -> LlmResponse:
        """agent-chatì— í…ìŠ¤íŠ¸ ìš”ì²­.
        
        connector: ì‚¬ìš©í•  ì»¤ë„¥í„° (ê¸°ë³¸: sequential-thinking)
          - "sequential-thinking": ë²”ìš© ì¶”ë¡ 
          - ë‹¤ë¥¸ ì»¤ë„¥í„°ë„ ê°€ëŠ¥ (academic-mcp ë“±)
        """
        full_prompt = prompt
        if system:
            full_prompt = f"[ì‹œìŠ¤í…œ ì§€ì‹œ]\n{system}\n\n[ìš”ì²­]\n{prompt}"
        
        async with httpx.AsyncClient(timeout=120.0) as client:
            resp = await client.post(
                f"{self.AGENT_CHAT_URL}/api/chat",
                json={
                    "text": full_prompt,
                    "connector": connector,
                }
            )
            data = resp.json()
        
        return LlmResponse(
            text=data.get("content", ""),
            provider="base44_http",
            model="base44_invokellm",
            tokens_in=None,   # Base44ê°€ í† í° ìˆ˜ë¥¼ ë°˜í™˜í•˜ì§€ ì•ŠìŒ
            tokens_out=None,
            cost_usd=0.0,     # ë¬´ë£Œ
            raw=data,
        )
    
    async def call_with_image(self, prompt, image, *,
                              image_mime="image/png", **kwargs) -> LlmResponse:
        """agent-chatì— ì´ë¯¸ì§€ ì²¨ë¶€ ìš”ì²­.
        
        agent-chatì˜ attachments ê¸°ëŠ¥ ì‚¬ìš©:
        - base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ ì „ì†¡
        - agent-chatì´ Base44 UploadFile â†’ InvokeLLM(file_urls) ì²˜ë¦¬
        """
        import base64
        
        attachment = {
            "name": "page_image.png",
            "type": image_mime,
            "data": base64.b64encode(image).decode("ascii"),
        }
        
        async with httpx.AsyncClient(timeout=120.0) as client:
            resp = await client.post(
                f"{self.AGENT_CHAT_URL}/api/chat",
                json={
                    "text": prompt,
                    "connector": "sequential-thinking",
                    "attachments": [attachment],
                }
            )
            data = resp.json()
        
        return LlmResponse(
            text=data.get("content", ""),
            provider="base44_http",
            model="base44_invokellm_vision",
            tokens_in=None,
            tokens_out=None,
            cost_usd=0.0,
            raw=data,
        )
```

### 4.2 Base44 Bridge (2ìˆœìœ„)

```python
# src/llm/providers/base44_bridge.py

class Base44BridgeProvider(BaseLlmProvider):
    """Node.js ë¸Œë¦¿ì§€ ìŠ¤í¬ë¦½íŠ¸ë¥¼ subprocessë¡œ ì‹¤í–‰í•˜ì—¬ Base44 SDK í˜¸ì¶œ.
    
    agent-chat ì„œë²„ê°€ ì•ˆ ë– ìˆì„ ë•Œì˜ ëŒ€ì•ˆ.
    Node.js í”„ë¡œì„¸ìŠ¤ë¥¼ 1íšŒì„±ìœ¼ë¡œ ì‹¤í–‰, JSON ê²°ê³¼ë¥¼ stdoutìœ¼ë¡œ ë°›ìŒ.
    
    í˜¸ì¶œ íë¦„:
      Python â†’ subprocess.run(["node", "invoke.js", ...])
            â†’ invoke.js â†’ Base44 SDK InvokeLLM
            â†’ stdout JSON â†’ Python íŒŒì‹±
    
    ì „ì œ:
      - Node.js 20+ ì„¤ì¹˜ë¨
      - backend-44 ë””ë ‰í† ë¦¬ê°€ ì„¤ì •ì— ì§€ì •ë¨
      - base44 login ì™„ë£Œ (í† í°ì´ ~/.base44/auth/auth.jsonì— ìˆìŒ)
    """
    
    provider_id = "base44_bridge"
    display_name = "Base44 (bridge)"
    supports_image = True
    
    async def is_available(self) -> bool:
        """Node.js + backend-44 + ì¸ì¦ í† í° ì¡´ì¬ í™•ì¸."""
        # 1. Node.js ì„¤ì¹˜ í™•ì¸
        try:
            result = await asyncio.create_subprocess_exec(
                "node", "--version",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
            await result.wait()
            if result.returncode != 0:
                return False
        except FileNotFoundError:
            return False
        
        # 2. bridge ìŠ¤í¬ë¦½íŠ¸ ì¡´ì¬ í™•ì¸
        bridge_path = self.config.get("base44_bridge_script")
        if not bridge_path or not Path(bridge_path).exists():
            return False
        
        # 3. Base44 ì¸ì¦ í† í° í™•ì¸
        auth_path = Path.home() / ".base44" / "auth" / "auth.json"
        if not auth_path.exists():
            return False
        
        return True
    
    async def call(self, prompt, *, system=None, response_format="text",
                   **kwargs) -> LlmResponse:
        """Node.js ë¸Œë¦¿ì§€ë¡œ InvokeLLM í˜¸ì¶œ."""
        bridge_script = self.config["base44_bridge_script"]
        
        input_data = json.dumps({
            "prompt": prompt,
            "system": system,
            "response_type": response_format,
        })
        
        proc = await asyncio.create_subprocess_exec(
            "node", bridge_script,
            stdin=asyncio.subprocess.PIPE,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        stdout, stderr = await proc.communicate(input_data.encode())
        
        if proc.returncode != 0:
            raise LlmProviderError(
                f"Base44 bridge ì‹¤í–‰ ì‹¤íŒ¨:\n{stderr.decode()}"
            )
        
        data = json.loads(stdout.decode())
        
        return LlmResponse(
            text=data.get("text", ""),
            provider="base44_bridge",
            model="base44_invokellm",
            tokens_in=None,
            tokens_out=None,
            cost_usd=0.0,
            raw=data,
        )
    
    async def call_with_image(self, prompt, image, *,
                              image_mime="image/png", **kwargs) -> LlmResponse:
        """ì´ë¯¸ì§€ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ â†’ bridgeì— ê²½ë¡œ ì „ë‹¬."""
        import tempfile, base64
        
        with tempfile.NamedTemporaryFile(
            suffix=".png", delete=False
        ) as tmp:
            tmp.write(image)
            tmp_path = tmp.name
        
        try:
            bridge_script = self.config["base44_bridge_vision_script"]
            
            input_data = json.dumps({
                "prompt": prompt,
                "image_path": tmp_path,
                "image_mime": image_mime,
            })
            
            proc = await asyncio.create_subprocess_exec(
                "node", bridge_script,
                stdin=asyncio.subprocess.PIPE,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
            stdout, stderr = await proc.communicate(input_data.encode())
            
            if proc.returncode != 0:
                raise LlmProviderError(
                    f"Base44 bridge vision ì‹¤í–‰ ì‹¤íŒ¨:\n{stderr.decode()}"
                )
            
            data = json.loads(stdout.decode())
            
            return LlmResponse(
                text=data.get("text", ""),
                provider="base44_bridge",
                model="base44_invokellm_vision",
                tokens_in=None,
                tokens_out=None,
                cost_usd=0.0,
                raw=data,
            )
        finally:
            Path(tmp_path).unlink(missing_ok=True)
```

### 4.3 Ollama (3ìˆœìœ„)

```python
# src/llm/providers/ollama.py

class OllamaProvider(BaseLlmProvider):
    """Ollama ë¡œì»¬ ì„œë²„(localhost:11434)ë¥¼ í†µí•œ LLM í˜¸ì¶œ.
    
    í´ë¼ìš°ë“œ ëª¨ë¸ë„ Ollamaê°€ í”„ë¡ì‹œ:
      - qwen3-vl:235b-cloud    â† ì´ë¯¸ì§€ ë¶„ì„ ê°€ëŠ¥ (ë¹„ì „ ëª¨ë¸)
      - kimi-k2.5:cloud
      - minimax-m2.5:cloud
      - glm-5:cloud
      - gemini-3-flash-preview:cloud
    
    í˜¸ì¶œ íë¦„:
      Python â†’ HTTP POST localhost:11434/api/generate
            â†’ Ollama â†’ í´ë¼ìš°ë“œ ëª¨ë¸ í”„ë¡ì‹œ
            â†’ ê²°ê³¼ ë°˜í™˜
    """
    
    provider_id = "ollama"
    display_name = "Ollama"
    supports_image = True  # qwen3-vl ë“± ë¹„ì „ ëª¨ë¸
    
    OLLAMA_URL = "http://localhost:11434"
    
    # ìš©ë„ë³„ ê¸°ë³¸ ëª¨ë¸
    DEFAULT_MODELS = {
        "text": "kimi-k2.5:cloud",           # ë²”ìš© í…ìŠ¤íŠ¸
        "vision": "qwen3-vl:235b-cloud",     # ì´ë¯¸ì§€ ë¶„ì„
        "translation": "glm-5:cloud",        # ë²ˆì—­
        "json": "gemini-3-flash-preview:cloud",  # JSON êµ¬ì¡°í™” ì¶œë ¥
    }
    
    async def is_available(self) -> bool:
        """Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸."""
        try:
            async with httpx.AsyncClient(timeout=2.0) as client:
                resp = await client.get(f"{self.OLLAMA_URL}/api/tags")
                return resp.status_code == 200
        except (httpx.ConnectError, httpx.TimeoutException):
            return False
    
    async def call(self, prompt, *, system=None, response_format="text",
                   model=None, purpose="text", **kwargs) -> LlmResponse:
        """Ollama APIë¡œ í…ìŠ¤íŠ¸ ìƒì„±.
        
        purpose: ìš©ë„ íŒíŠ¸ ("text", "translation", "json")
                 â†’ ìš©ë„ë³„ ê¸°ë³¸ ëª¨ë¸ ìë™ ì„ íƒ
        """
        selected_model = model or self.DEFAULT_MODELS.get(purpose, "kimi-k2.5:cloud")
        
        payload = {
            "model": selected_model,
            "prompt": prompt,
            "stream": False,
        }
        if system:
            payload["system"] = system
        if response_format == "json":
            payload["format"] = "json"
        
        async with httpx.AsyncClient(timeout=120.0) as client:
            resp = await client.post(
                f"{self.OLLAMA_URL}/api/generate",
                json=payload,
            )
            data = resp.json()
        
        return LlmResponse(
            text=data.get("response", ""),
            provider="ollama",
            model=selected_model,
            tokens_in=data.get("prompt_eval_count"),
            tokens_out=data.get("eval_count"),
            cost_usd=0.0,  # í´ë¼ìš°ë“œ ëª¨ë¸ë„ Ollama í”„ë¡ì‹œ ë¹„ìš©ì€ ë³„ë„ ì¶”ì 
            raw=data,
        )
    
    async def call_with_image(self, prompt, image, *,
                              image_mime="image/png", model=None,
                              **kwargs) -> LlmResponse:
        """Ollama ë¹„ì „ ëª¨ë¸ë¡œ ì´ë¯¸ì§€ ë¶„ì„.
        
        qwen3-vl:235b-cloudê°€ ê¸°ë³¸ ë¹„ì „ ëª¨ë¸.
        Ollama APIëŠ” images í•„ë“œì— base64 ë°°ì—´ì„ ë°›ëŠ”ë‹¤.
        """
        import base64
        
        selected_model = model or self.DEFAULT_MODELS["vision"]
        
        payload = {
            "model": selected_model,
            "prompt": prompt,
            "images": [base64.b64encode(image).decode("ascii")],
            "stream": False,
        }
        
        async with httpx.AsyncClient(timeout=180.0) as client:
            resp = await client.post(
                f"{self.OLLAMA_URL}/api/generate",
                json=payload,
            )
            data = resp.json()
        
        return LlmResponse(
            text=data.get("response", ""),
            provider="ollama",
            model=selected_model,
            tokens_in=data.get("prompt_eval_count"),
            tokens_out=data.get("eval_count"),
            cost_usd=0.0,
            raw=data,
        )
```

### 4.4 ì§ì ‘ API (4ìˆœìœ„) â€” Anthropic ì˜ˆì‹œ

```python
# src/llm/providers/anthropic.py

class AnthropicProvider(BaseLlmProvider):
    """Anthropic Claude API ì§ì ‘ í˜¸ì¶œ.
    
    ìµœí›„ ìˆ˜ë‹¨. ìœ ë£Œì§€ë§Œ ê°€ì¥ ì•ˆì •ì .
    ê³ ì „ í•œë¬¸ ë¶„ì„ì—ëŠ” Claudeê°€ ê°€ì¥ ì •í™•í•  ìˆ˜ ìˆë‹¤.
    """
    
    provider_id = "anthropic"
    display_name = "Claude (Anthropic)"
    supports_image = True
    
    DEFAULT_MODEL = "claude-sonnet-4-20250514"
    
    async def is_available(self) -> bool:
        """ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸."""
        return bool(self.config.get_api_key("anthropic"))
    
    async def call(self, prompt, *, system=None, model=None,
                   max_tokens=4096, **kwargs) -> LlmResponse:
        import anthropic
        
        client = anthropic.AsyncAnthropic(
            api_key=self.config.get_api_key("anthropic")
        )
        
        messages = [{"role": "user", "content": prompt}]
        
        response = await client.messages.create(
            model=model or self.DEFAULT_MODEL,
            max_tokens=max_tokens,
            system=system or "",
            messages=messages,
        )
        
        text = response.content[0].text
        
        return LlmResponse(
            text=text,
            provider="anthropic",
            model=response.model,
            tokens_in=response.usage.input_tokens,
            tokens_out=response.usage.output_tokens,
            cost_usd=self._estimate_cost(response),
            raw={"id": response.id},
        )
    
    # call_with_imageë„ ìœ ì‚¬í•˜ê²Œ êµ¬í˜„ (content blockì— image ì¶”ê°€)
```

---

## 5. Node.js Bridge ìŠ¤í¬ë¦½íŠ¸

### 5.1 invoke.js (í…ìŠ¤íŠ¸ ì „ìš©)

```javascript
// src/llm/bridge/invoke.js
// 
// Pythonì—ì„œ subprocessë¡œ ì‹¤í–‰ë˜ëŠ” 1íšŒì„± ìŠ¤í¬ë¦½íŠ¸.
// stdinìœ¼ë¡œ JSON ì…ë ¥ì„ ë°›ê³ , stdoutìœ¼ë¡œ JSON ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.
//
// ì‚¬ìš©: echo '{"prompt":"..."}' | node invoke.js
// ì „ì œ: backend-44ì˜ client.jsë¥¼ importí•  ìˆ˜ ìˆì–´ì•¼ í•¨

import { readFileSync } from 'fs';
import { getBase44Client, ensureAuth } from '../../../backend-44/src/client.js';

async function main() {
  // stdinì—ì„œ ì…ë ¥ ì½ê¸°
  const chunks = [];
  for await (const chunk of process.stdin) {
    chunks.push(chunk);
  }
  const input = JSON.parse(Buffer.concat(chunks).toString('utf8'));
  
  const { prompt, system, response_type } = input;
  
  // Base44 ì¸ì¦ í™•ì¸
  ensureAuth();
  
  const base44 = getBase44Client();
  
  // ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ìˆìœ¼ë©´ í”„ë¡¬í”„íŠ¸ ì•ì— ì¶”ê°€
  const fullPrompt = system 
    ? `[ì‹œìŠ¤í…œ ì§€ì‹œ]\n${system}\n\n[ìš”ì²­]\n${prompt}`
    : prompt;
  
  const result = await base44.integrations.Core.InvokeLLM({
    prompt: fullPrompt,
    response_type: response_type || 'text',
  });
  
  // ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ stdoutì— ì¶œë ¥
  const text = typeof result === 'string' 
    ? result 
    : (result?.content || JSON.stringify(result));
  
  const output = { text, provider: 'base44_bridge', raw: result };
  process.stdout.write(JSON.stringify(output));
}

main().catch(e => {
  process.stderr.write(JSON.stringify({ error: e.message }));
  process.exit(1);
});
```

### 5.2 invoke_vision.js (ì´ë¯¸ì§€ ë¶„ì„)

```javascript
// src/llm/bridge/invoke_vision.js
//
// ì´ë¯¸ì§€ íŒŒì¼ì„ Base44ì— ì—…ë¡œë“œí•œ í›„ InvokeLLM(file_urls)ë¡œ ë¶„ì„.
// stdin: {"prompt": "...", "image_path": "/tmp/xxx.png"}
// stdout: {"text": "...", "provider": "base44_bridge"}

import { readFileSync } from 'fs';
import { getBase44Client, ensureAuth } from '../../../backend-44/src/client.js';

async function main() {
  const chunks = [];
  for await (const chunk of process.stdin) {
    chunks.push(chunk);
  }
  const input = JSON.parse(Buffer.concat(chunks).toString('utf8'));
  
  const { prompt, image_path, image_mime } = input;
  
  ensureAuth();
  const base44 = getBase44Client();
  
  // ì´ë¯¸ì§€ íŒŒì¼ ì½ê¸° â†’ File ê°ì²´ ìƒì„±
  const imageBuffer = readFileSync(image_path);
  const fileName = image_path.split(/[\\/]/).pop();
  
  let fileObj;
  if (typeof globalThis.File === 'function') {
    fileObj = new globalThis.File([imageBuffer], fileName, {
      type: image_mime || 'image/png'
    });
  } else {
    fileObj = new globalThis.Blob([imageBuffer], {
      type: image_mime || 'image/png'
    });
    fileObj.name = fileName;
  }
  
  // Base44ì— ì—…ë¡œë“œ
  const uploadResult = await base44.integrations.Core.UploadFile({
    file: fileObj,
  });
  
  if (!uploadResult?.file_url) {
    throw new Error('íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: file_urlì´ ë°˜í™˜ë˜ì§€ ì•ŠìŒ');
  }
  
  // InvokeLLMì— file_urlsë¡œ ì „ë‹¬
  const result = await base44.integrations.Core.InvokeLLM({
    prompt,
    file_urls: [uploadResult.file_url],
  });
  
  const text = typeof result === 'string'
    ? result
    : (result?.content || JSON.stringify(result));
  
  process.stdout.write(JSON.stringify({
    text,
    provider: 'base44_bridge',
    file_url: uploadResult.file_url,
    raw: result,
  }));
}

main().catch(e => {
  process.stderr.write(JSON.stringify({ error: e.message }));
  process.exit(1);
});
```

---

## 6. ì„¤ì • ê´€ë¦¬

### 6.1 config.py

```python
# src/llm/config.py

class LlmConfig:
    """LLM ì„¤ì • ê´€ë¦¬.
    
    ì„¤ì • ìš°ì„ ìˆœìœ„:
      1. í™˜ê²½ë³€ìˆ˜ (.env)
      2. ì„œê³  ì„¤ì • íŒŒì¼ (~/.classical-text-platform/llm_config.json)
      3. ê¸°ë³¸ê°’
    """
    
    def __init__(self, library_root: Path | None = None):
        self._env = dotenv.dotenv_values(library_root / ".env") if library_root else {}
        self._config = self._load_global_config()
    
    def get_api_key(self, provider: str) -> str | None:
        """API í‚¤ ì¡°íšŒ. í™˜ê²½ë³€ìˆ˜ â†’ ì„¤ì • íŒŒì¼ â†’ None."""
        env_keys = {
            "anthropic": "ANTHROPIC_API_KEY",
            "openai": "OPENAI_API_KEY",
            "gemini": "GOOGLE_API_KEY",
            "base44": "BASE44_TOKEN",
        }
        env_name = env_keys.get(provider)
        if env_name:
            return os.environ.get(env_name) or self._env.get(env_name)
        return None
    
    def get(self, key: str, default=None):
        """ì¼ë°˜ ì„¤ì •ê°’ ì¡°íšŒ."""
        return self._config.get(key, default)
    
    # ì„¤ì • í•­ëª©ë“¤
    DEFAULTS = {
        "provider_priority": [
            "base44_http", "base44_bridge", "ollama",
            "anthropic", "openai", "gemini"
        ],
        "agent_chat_url": "http://127.0.0.1:8787",
        "ollama_url": "http://localhost:11434",
        "base44_bridge_script": None,          # backend-44 ê²½ë¡œ (ìˆ˜ë™ ì„¤ì •)
        "base44_bridge_vision_script": None,
        "ollama_default_model": "kimi-k2.5:cloud",
        "ollama_vision_model": "qwen3-vl:235b-cloud",
        "anthropic_default_model": "claude-sonnet-4-20250514",
        "monthly_budget_usd": 10.0,            # ì›”ê°„ ì˜ˆì‚° (ìœ ë£Œ APIìš©)
    }
```

### 6.2 .env ì˜ˆì‹œ

```env
# LLM ì„¤ì • â€” classical-text-platform/.env
# ì´ íŒŒì¼ì€ .gitignoreì— í¬í•¨ë˜ì–´ì•¼ í•œë‹¤!

# Base44 (1Â·2ìˆœìœ„ â€” ë¬´ë£Œ)
BASE44_TOKEN=your_base44_token_here

# backend-44 ê²½ë¡œ (2ìˆœìœ„ bridgeìš©)
BASE44_BACKEND_PATH=C:\Users\junto\Downloads\head-repo\hw725\backend-44

# Ollama (3ìˆœìœ„ â€” ë¡œì»¬ ì„œë²„)
# Ollamaê°€ localhost:11434ì—ì„œ ì‹¤í–‰ ì¤‘ì´ë©´ ìë™ ê°ì§€

# ì§ì ‘ API (4ìˆœìœ„ â€” ìœ ë£Œ)
ANTHROPIC_API_KEY=sk-ant-...
# OPENAI_API_KEY=sk-...
# GOOGLE_API_KEY=AIza...

# ì˜ˆì‚°
LLM_MONTHLY_BUDGET_USD=10.0
```

---

## 7. ìš©ë„ë³„ ëª¨ë¸ ì„ íƒ ì „ëµ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ìš©ë„                    â”‚ ëª¨ë¸ ì„ íƒ                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ë ˆì´ì•„ì›ƒ ë¶„ì„ (10-2)    â”‚ ë¹„ì „ í•„ìˆ˜:                        â”‚
â”‚  ì´ë¯¸ì§€ â†’ LayoutBlock   â”‚  1. Base44 InvokeLLM + UploadFileâ”‚
â”‚                        â”‚  2. Ollama qwen3-vl:235b-cloud  â”‚
â”‚                        â”‚  3. Claude claude-sonnet-4       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ë²ˆì—­ (11-2)            â”‚ í…ìŠ¤íŠ¸:                           â”‚
â”‚  í•œë¬¸ â†’ í˜„ëŒ€í•œêµ­ì–´       â”‚  1. Base44 InvokeLLM             â”‚
â”‚                        â”‚  2. Ollama glm-5:cloud           â”‚
â”‚                        â”‚  3. Claude (í•œë¬¸ì— ê°•í•¨)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ì£¼ì„ ìë™ ìƒì„± (11-3)   â”‚ í…ìŠ¤íŠ¸:                           â”‚
â”‚  ì¸ë¬¼/ì§€ëª…/ì „ê±° ì‹ë³„     â”‚  1. Base44 InvokeLLM             â”‚
â”‚                        â”‚  2. Ollama kimi-k2.5:cloud       â”‚
â”‚                        â”‚  3. Claude                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ JSON êµ¬ì¡°í™” ì¶œë ¥        â”‚ JSON ëª¨ë“œ ì§€ì› ëª¨ë¸:              â”‚
â”‚  í”„ë¡¬í”„íŠ¸ â†’ JSON        â”‚  1. Base44 (response_type: json) â”‚
â”‚                        â”‚  2. Ollama gemini-3-flash:cloud  â”‚
â”‚                        â”‚  3. Claude (JSON mode)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ OCR ë³´ì¡° (10-1)        â”‚ ë¹„ì „ í•„ìˆ˜:                        â”‚
â”‚  ì €í’ˆì§ˆ ì´ë¯¸ì§€ íŒë…     â”‚  1. Base44 + UploadFile          â”‚
â”‚                        â”‚  2. Ollama qwen3-vl              â”‚
â”‚                        â”‚  3. Claude Vision                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 8. ë¹„ìš© ì¶”ì 

```python
# src/llm/usage_tracker.py

class UsageTracker:
    """LLM ì‚¬ìš©ëŸ‰ ì¶”ì .
    
    ì„œê³ ë³„ llm_usage_log.jsonlì— ë§¤ í˜¸ì¶œ ê¸°ë¡.
    ë¬´ë£Œ provider(Base44, Ollama)ë„ ê¸°ë¡í•˜ì—¬ ì‚¬ìš© íŒ¨í„´ ë¶„ì„.
    """
    
    def log(self, response: LlmResponse, purpose: str = ""):
        """í˜¸ì¶œ ê¸°ë¡ ì¶”ê°€."""
        entry = {
            "ts": datetime.now().isoformat(),
            "provider": response.provider,
            "model": response.model,
            "tokens_in": response.tokens_in,
            "tokens_out": response.tokens_out,
            "cost_usd": response.cost_usd or 0.0,
            "purpose": purpose,  # "layout_analysis", "translation", ...
        }
        # jsonl íŒŒì¼ì— append
    
    def log_comparison(self, purpose, targets, results):
        """ë¹„êµ ëª¨ë“œ í˜¸ì¶œ ê¸°ë¡.
        
        ì–´ë–¤ ëª¨ë¸ë“¤ì„ ë¹„êµí–ˆëŠ”ì§€, ê° ê²°ê³¼ì˜ ê¸¸ì´Â·í† í° ë“±ì„ ê¸°ë¡.
        ë‚˜ì¤‘ì— "ì–´ë–¤ ëª¨ë¸ì´ ë²ˆì—­ì— ê°€ì¥ ì¢‹ì•˜ë‚˜" ë¶„ì„ì— ì‚¬ìš©.
        """
        entry = {
            "ts": datetime.now().isoformat(),
            "type": "comparison",
            "purpose": purpose,
            "targets": [
                {"provider": pid, "model": model}
                for pid, model in targets
            ],
            "results": [
                {
                    "provider": r.provider if isinstance(r, LlmResponse) else None,
                    "model": r.model if isinstance(r, LlmResponse) else None,
                    "text_length": len(r.text) if isinstance(r, LlmResponse) else 0,
                    "error": str(r) if isinstance(r, Exception) else None,
                }
                for r in results
            ],
        }
        # jsonl íŒŒì¼ì— append
    
    def get_monthly_summary(self) -> dict:
        """ì´ë²ˆ ë‹¬ ì‚¬ìš©ëŸ‰ ìš”ì•½."""
        return {
            "total_calls": 42,
            "total_cost_usd": 1.23,
            "by_provider": {
                "base44_http": {"calls": 30, "cost": 0.0},
                "ollama": {"calls": 8, "cost": 0.0},
                "anthropic": {"calls": 4, "cost": 1.23},
            },
            "by_purpose": {
                "layout_analysis": 15,
                "translation": 20,
                "annotation": 7,
            },
            "budget_remaining_usd": 8.77,
        }
```

---

## 9. ëª¨ë¸ ì„ íƒ ë° ë¹„êµ GUI

### 9.1 ëª¨ë¸ ì„ íƒ ë“œë¡­ë‹¤ìš´

ëª¨ë“  LLM ê¸°ëŠ¥(ë ˆì´ì•„ì›ƒ ë¶„ì„, ë²ˆì—­, ì£¼ì„)ì˜ UIì— ëª¨ë¸ ì„ íƒ ì˜µì…˜ì„ ë‘”ë‹¤.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  [AI ë¶„ì„]                                   â”‚
â”‚                                              â”‚
â”‚  ëª¨ë¸: [ğŸ”„ ìë™ (í´ë°±ìˆœì„œ)            â–¼]     â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚        â”‚ ğŸ”„ ìë™ (í´ë°±ìˆœì„œ)            â”‚ â† ê¸°ë³¸ê°’     â”‚
â”‚        â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚      â”‚
â”‚        â”‚ ğŸŸ¢ Base44 InvokeLLM         â”‚ â† ë¬´ë£Œ     â”‚
â”‚        â”‚ ğŸŸ¢ Ollama: qwen3-vl (ë¹„ì „)  â”‚ â† ë¬´ë£Œ     â”‚
â”‚        â”‚ ğŸŸ¢ Ollama: kimi-k2.5        â”‚ â† ë¬´ë£Œ     â”‚
â”‚        â”‚ ğŸŸ¢ Ollama: glm-5            â”‚ â† ë¬´ë£Œ     â”‚
â”‚        â”‚ ğŸŸ¢ Ollama: minimax-m2.5     â”‚ â† ë¬´ë£Œ     â”‚
â”‚        â”‚ ğŸŸ¢ Ollama: gemini-3-flash   â”‚ â† ë¬´ë£Œ     â”‚
â”‚        â”‚ âš« Claude sonnet-4          â”‚ â† ìœ ë£Œ, í‚¤ ë¯¸ì„¤ì •     â”‚
â”‚        â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚      â”‚
â”‚        â”‚ ğŸ”¬ ë¹„êµ ëª¨ë“œ                  â”‚ â† ì „ì²´ ë¹„êµ     â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                              â”‚
â”‚  ğŸŸ¢ = ì‚¬ìš© ê°€ëŠ¥   âš« = ì‚¬ìš© ë¶ˆê°€             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- `GET /api/llm/models` â†’ `router.get_available_models()` í˜¸ì¶œ
- ë“œë¡­ë‹¤ìš´ì€ ê°€ìš© ëª¨ë¸ë§Œ ì„ íƒ ê°€ëŠ¥, ë¶ˆê°€í•œ ê²ƒì€ íšŒìƒ‰ + ì‚¬ìœ  í‘œì‹œ
- ì„ íƒí•œ ëª¨ë¸ì€ API í˜¸ì¶œ ì‹œ `force_provider` + `force_model`ë¡œ ì „ë‹¬

### 9.2 ë¹„êµ ëª¨ë“œ UI

"ğŸ”¬ ë¹„êµ ëª¨ë“œ"ë¥¼ ì„ íƒí•˜ë©´, ê°™ì€ ì…ë ¥ì„ ì—¬ëŸ¬ ëª¨ë¸ì— ë™ì‹œì— ë³´ë‚´ê³  ê²°ê³¼ë¥¼ ë‚˜ë€íˆ í‘œì‹œ.

```
â”Œâ”€ ë¹„êµ ê²°ê³¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                          â”‚
â”‚  ì…ë ¥: [í˜ì´ì§€ 3 ì´ë¯¸ì§€ â€” ë ˆì´ì•„ì›ƒ ë¶„ì„]                   â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Base44       â”‚ Ollama       â”‚ Claude       â”‚          â”‚
â”‚  â”‚ InvokeLLM    â”‚ qwen3-vl     â”‚ sonnet-4     â”‚          â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”‚
â”‚  â”‚ ë¸”ë¡ 5ê°œ      â”‚ ë¸”ë¡ 6ê°œ      â”‚ ë¸”ë¡ 5ê°œ      â”‚          â”‚
â”‚  â”‚ ë³¸ë¬¸ 2       â”‚ ë³¸ë¬¸ 2       â”‚ ë³¸ë¬¸ 2       â”‚          â”‚
â”‚  â”‚ ì£¼ì„ 2       â”‚ ì£¼ì„ 3       â”‚ ì£¼ì„ 2       â”‚          â”‚
â”‚  â”‚ íŒì‹¬ì œ 1     â”‚ íŒì‹¬ì œ 1     â”‚ íŒì‹¬ì œ 1     â”‚          â”‚
â”‚  â”‚              â”‚ ì¥ì°¨ ì¶”ê°€ â˜…  â”‚              â”‚          â”‚
â”‚  â”‚              â”‚              â”‚              â”‚          â”‚
â”‚  â”‚ â± 2.1ì´ˆ      â”‚ â± 3.4ì´ˆ      â”‚ â± 1.8ì´ˆ      â”‚          â”‚
â”‚  â”‚ ğŸ’° ë¬´ë£Œ       â”‚ ğŸ’° ë¬´ë£Œ       â”‚ ğŸ’° $0.003    â”‚          â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”‚
â”‚  â”‚ [ì´ ê²°ê³¼ ì±„íƒ]â”‚ [ì´ ê²°ê³¼ ì±„íƒ]â”‚ [ì´ ê²°ê³¼ ì±„íƒ]â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                          â”‚
â”‚  [ì „ì²´ ì·¨ì†Œ]                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- "ì´ ê²°ê³¼ ì±„íƒ" â†’ í•´ë‹¹ ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ Draftë¡œ ì „í™˜ â†’ ê¸°ì¡´ Review ì›Œí¬í”Œë¡œìš°ë¡œ
- ë¹„êµ ê²°ê³¼ëŠ” usage_trackerì— ê¸°ë¡ â†’ "ì–´ë–¤ ëª¨ë¸ì„ ìì£¼ ì±„íƒí–ˆë‚˜" í†µê³„ ê°€ëŠ¥

### 9.3 í’ˆì§ˆ í‰ê°€ ê¸°ë¡

Draftë¥¼ reviewí•  ë•Œ ê°„ë‹¨í•œ í’ˆì§ˆ í‰ê°€ë¥¼ ê¸°ë¡í•  ìˆ˜ ìˆë‹¤.

```python
@dataclass
class LlmDraft:
    # ... ê¸°ì¡´ í•„ë“œë“¤ ...
    
    # í’ˆì§ˆ í‰ê°€ (review ì‹œ ê¸°ë¡)
    quality_rating: int | None = None    # 1~5ì 
    quality_notes: str | None = None     # "ì£¼ì„ ì˜ì—­ì„ ë¹ ëœ¨ë ¸ìŒ"
    
    # ë¹„êµ ëª¨ë“œì—ì„œ ì±„íƒëœ ê²½ìš°
    compared_with: list[str] | None = None  # ["base44_http", "anthropic"]
    chosen_reason: str | None = None        # "ë¸”ë¡ êµ¬ë¶„ì´ ê°€ì¥ ì •í™•"
```

ì´ ë°ì´í„°ê°€ ìŒ“ì´ë©´ ë‚˜ì¤‘ì— ìš©ë„ë³„ ìµœì  ëª¨ë¸ì„ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨í•  ìˆ˜ ìˆë‹¤:

```
"ë ˆì´ì•„ì›ƒ ë¶„ì„ 30íšŒ ì¤‘:
  - Ollama qwen3-vl ì±„íƒ 18íšŒ (í‰ê·  4.2ì )
  - Base44 ì±„íƒ 8íšŒ (í‰ê·  3.5ì )
  - Claude ì±„íƒ 4íšŒ (í‰ê·  4.5ì , ë¹„ìš© ëŒ€ë¹„ íš¨ìœ¨ì€ ë‚®ìŒ)
â†’ ê¸°ë³¸ ëª¨ë¸ì„ qwen3-vlë¡œ ë³€ê²½ ê¶Œì¥"
```

### 9.4 Ollama ëª¨ë¸ ëª©ë¡ ë™ì  ì¡°íšŒ

Ollama providerì— ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ê¸°ëŠ¥ ì¶”ê°€:

```python
class OllamaProvider(BaseLlmProvider):
    
    async def list_models(self) -> list[dict]:
        """Ollamaì— ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ.
        
        GET localhost:11434/api/tags â†’ ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡
        í´ë¼ìš°ë“œ ëª¨ë¸(:cloud ì ‘ë¯¸ì‚¬)ë„ í¬í•¨.
        
        ë°˜í™˜ ì˜ˆì‹œ:
        [
            {"name": "qwen3-vl:235b-cloud", "size": "N/A", "vision": True},
            {"name": "kimi-k2.5:cloud", "size": "N/A", "vision": False},
            {"name": "llama3.2:3b", "size": "2.0 GB", "vision": False},
        ]
        """
        async with httpx.AsyncClient(timeout=5.0) as client:
            resp = await client.get(f"{self.OLLAMA_URL}/api/tags")
            data = resp.json()
        
        models = []
        for m in data.get("models", []):
            name = m.get("name", "")
            models.append({
                "name": name,
                "size": m.get("size", "N/A"),
                "vision": any(kw in name.lower() for kw in ["vl", "vision", "llava"]),
            })
        return models
```

---

## 9. Phase 10-2 ì„¸ì…˜ ì§€ì‹œë¬¸ (ì—…ë°ì´íŠ¸)

```
# ì»¨í…ìŠ¤íŠ¸

í”„ë¡œì íŠ¸: ê³ ì „ í…ìŠ¤íŠ¸ ë””ì§€í„¸ ì„œê³  í”Œë«í¼
Phase 10-1 OCR ì™„ë£Œ. ì´ë²ˆì€ LLM ì•„í‚¤í…ì²˜.

CLAUDE.mdë¥¼ ë¨¼ì € ì½ì–´.
docs/phase10_12_design.md â€” Phase 10-2 ì„¹ì…˜ ì½ì–´.
docs/llm_architecture_design.md â€” ì´ ë¬¸ì„œë¥¼ ì½ì–´. LLM í˜¸ì¶œ ì•„í‚¤í…ì²˜ ìƒì„¸ ì„¤ê³„.

## ì´ë²ˆ ëª©í‘œ: Phase 10-2 â€” LLM í˜¸ì¶œ ì•„í‚¤í…ì²˜ + ë ˆì´ì•„ì›ƒ ë¶„ì„

### í•µì‹¬: 4ë‹¨ í´ë°± LLM Router

ìš°ì„ ìˆœìœ„:
1. Base44 InvokeLLM via agent-chat HTTP (localhost:8787)
2. Base44 InvokeLLM via Node.js bridge (subprocess)
3. Ollama í´ë¼ìš°ë“œ ëª¨ë¸ (localhost:11434)
4. ì§ì ‘ API (Anthropic/OpenAI/Gemini)

ëª¨ë“  LLM í˜¸ì¶œì€ src/llm/router.pyë¥¼ í†µí•´ì•¼ í•œë‹¤.
providerë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì§€ ì•ŠëŠ”ë‹¤.

## ì‘ì—… ìˆœì„œ

### ì‘ì—… 1: Provider ì¶”ìƒ í´ë˜ìŠ¤ + LlmResponse

src/llm/providers/base.py:
- BaseLlmProvider: is_available(), call(), call_with_image()
- LlmResponse: text, provider, model, tokens, cost, raw
- LlmProviderError, LlmUnavailableError

### ì‘ì—… 2: Base44 HTTP Provider (1ìˆœìœ„)

src/llm/providers/base44_http.py:
- localhost:8787/api/chatë¡œ POST
- is_available: /api/meta GET í—¬ìŠ¤ì²´í¬ (timeout 2ì´ˆ)
- call: text â†’ connector="sequential-thinking"
- call_with_image: attachmentsì— base64 ì´ë¯¸ì§€ ì²¨ë¶€
- ì˜ì¡´ì„±: httpx (uv add httpx)

### ì‘ì—… 3: Node.js Bridge Provider (2ìˆœìœ„)

src/llm/providers/base44_bridge.py:
- asyncio.create_subprocess_execë¡œ node invoke.js ì‹¤í–‰
- stdinìœ¼ë¡œ JSON ì…ë ¥, stdoutì—ì„œ JSON ê²°ê³¼ íŒŒì‹±
- is_available: node --version + bridge ìŠ¤í¬ë¦½íŠ¸ ì¡´ì¬ + ~/.base44/auth í™•ì¸

src/llm/bridge/invoke.js:
- backend-44ì˜ src/client.jsë¥¼ import
- stdin JSON â†’ InvokeLLM â†’ stdout JSON

src/llm/bridge/invoke_vision.js:
- ì´ë¯¸ì§€ ê²½ë¡œ ë°›ì•„ì„œ UploadFile â†’ InvokeLLM(file_urls)

### ì‘ì—… 4: Ollama Provider (3ìˆœìœ„)

src/llm/providers/ollama.py:
- localhost:11434/api/generate POST
- ìš©ë„ë³„ ê¸°ë³¸ ëª¨ë¸:
  - text: kimi-k2.5:cloud
  - vision: qwen3-vl:235b-cloud
  - translation: glm-5:cloud
  - json: gemini-3-flash-preview:cloud
- call_with_image: images í•„ë“œì— base64 ë°°ì—´

### ì‘ì—… 5: Anthropic Provider (4ìˆœìœ„)

src/llm/providers/anthropic.py:
- anthropic Python SDK (uv add anthropic)
- ê¸°ë³¸ ëª¨ë¸: claude-sonnet-4-20250514

### ì‘ì—… 6: Router + Config

src/llm/router.py:
- LlmRouter: providers ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœì„œëŒ€ë¡œ ì‹œë„
- call(), call_with_image() â€” ì²« ì„±ê³µ provider ê²°ê³¼ ë°˜í™˜
- ì „ë¶€ ì‹¤íŒ¨ ì‹œ LlmUnavailableError (ì—ëŸ¬ ë©”ì‹œì§€ì— ê° provider ì‹¤íŒ¨ ì´ìœ )

src/llm/config.py:
- .envì—ì„œ API í‚¤, backend-44 ê²½ë¡œ, ì›”ê°„ ì˜ˆì‚° ì½ê¸°
- ê¸°ë³¸ ì„¤ì •ê°’ (DEFAULT_MODELS ë“±)

### ì‘ì—… 7: ë¹„ìš© ì¶”ì 

src/llm/usage_tracker.py:
- ì„œê³ ë³„ llm_usage_log.jsonl
- log(response, purpose) â€” ë§¤ í˜¸ì¶œ ê¸°ë¡
- get_monthly_summary() â€” ì›”ê°„ ìš”ì•½

### ì‘ì—… 8: Draft ëª¨ë¸

src/llm/draft.py:
- LlmDraft: draft_id, purpose, status (pendingâ†’accepted/modified/rejected)
- Draft â†’ Review â†’ Commit íŒ¨í„´ì˜ ê¸°ë°˜

### ì‘ì—… 9: ë ˆì´ì•„ì›ƒ ë¶„ì„ (Draft íŒ¨í„´ ì²« ì ìš©)

src/llm/prompts/layout_analysis.yaml:
- ì´ë¯¸ì§€ â†’ LayoutBlock ì œì•ˆ JSON
- block_type ëª©ë¡ í¬í•¨

src/core/layout_analyzer.py:
- analyze_page_layout() â†’ LlmDraft (status: pending)
- commit_layout_draft() â†’ layout_page.json ì €ì¥ + git commit

### ì‘ì—… 10: API ì—”ë“œí¬ì¸íŠ¸

POST /api/documents/{doc_id}/pages/{page}/layout/analyze
POST /api/llm/drafts/{draft_id}/commit
GET /api/llm/usage
POST /api/llm/config

GET /api/llm/status â€” ê° provider ê°€ìš© ìƒíƒœ ì¡°íšŒ
  ì‘ë‹µ: {
    "base44_http": {"available": true, "url": "localhost:8787"},
    "base44_bridge": {"available": true, "node": "v20.19.0"},
    "ollama": {"available": true, "models": ["kimi-k2.5:cloud", ...]},
    "anthropic": {"available": false, "reason": "API í‚¤ ë¯¸ì„¤ì •"}
  }

### ì‘ì—… 11: GUI â€” AI ë¶„ì„ + Provider ìƒíƒœ

layout-editor.js:
- "AI ë¶„ì„" ë²„íŠ¼ â†’ POST /analyze
- ì œì•ˆ ë¸”ë¡ì„ ì ì„ ìœ¼ë¡œ í‘œì‹œ
- ë¸”ë¡ë³„ [âœ… ìŠ¹ì¸] [âœï¸ ìˆ˜ì •] [âŒ ì‚­ì œ]
- "ì „ì²´ í™•ì •" â†’ POST /commit

ì„¤ì • ë˜ëŠ” ì‚¬ì´ë“œë°”:
- LLM ìƒíƒœ í‘œì‹œ: ğŸŸ¢ Base44 | ğŸŸ¢ Ollama | âš« Claude
- ì´ë²ˆ ë‹¬ ë¹„ìš©: $X.XX / $10.00

### ì‘ì—… 12: í†µí•© í…ŒìŠ¤íŠ¸

1. routerê°€ providerë¥¼ ìˆœì„œëŒ€ë¡œ ì‹œë„í•˜ëŠ”ì§€ (mock)
2. agent-chatì´ ì£½ì—ˆì„ ë•Œ ë‹¤ìŒ providerë¡œ í´ë°±í•˜ëŠ”ì§€
3. ë ˆì´ì•„ì›ƒ ë¶„ì„ â†’ Draft â†’ Review â†’ Commit ì „ì²´ íë¦„

ì»¤ë°‹: "feat: Phase 10-2 â€” LLM 4ë‹¨ í´ë°± ì•„í‚¤í…ì²˜ + ë ˆì´ì•„ì›ƒ ë¶„ì„"
```
